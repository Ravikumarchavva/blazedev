import Link from 'next/link';

---

# Predicting Car Prices with Linear Models

*Date: June 2024*

Github : <Link href='https://github.com/Ravikumarchavva/ML_Playground/tree/main/week1_regressionAnalysis' target="_blank"> RegressionAnalysis</Link>
## Objective

Our goal was to predict car prices using various linear models and machine learning techniques. This post summarizes our journey from start to finish, highlighting the key steps, technologies, and models used to build a robust car price prediction model.

---

## Overview
![Data Exploration Report](/projects/carPrice/carOverview.JPG)

---

### Start: Data Exploration and Preprocessing

We began by exploring the dataset to understand its structure. Preprocessing involved handling missing values, outliers, and categorical variables, ensuring the data was clean and ready for modeling. Initial models, such as Linear Regression, were trained to establish a baseline.

![Data Exploration Report](/projects/carPrice/Day1-report.jpg)

**Power BI Report**  
![Power BI Report](/projects/carPrice/carpriceReport_page-0001.jpg)

**Key Insights:**
- Wagon, hatchback cars are tend to have lower price.
- Having more cylinders tend to have higher price.
- Jaguar, Porcshe, Buick cars are tend to have higher price.
- Cars with higher horsepower tend to have higher price.

**Technologies Used:**
- **Language:** Python
- **Libraries:** Polars, NumPy, Scikit-learn
- **Visualization:** Matplotlib, Seaborn
- **Development Environment:** Jupyter Notebook
- **Development**: Docker, Google Cloud Run

---

### Feature Engineering and Naive Modeling

We then focused on feature engineering, creating new features like `carspace`, `averagempg`, and `performancebalance` to capture more information. Simple model like Linear Regression is implemented to establish a baseline performance.


**Key Takeaways:**
- Thoughtful feature engineering can significantly enhance model accuracy.
- Baseline models provide a useful point of comparison for future enhancements.

---

### Combating Overfitting with Regularization

To prevent overfitting, we introduced regularization techniques such as Lasso and Ridge Regression. These methods reduced model complexity while maintaining performance, ensuring that the models could generalize well to unseen data.

**Models Used:**
- Lasso Regression
- Ridge Regression
- ElasticNet

**Key Insights:**
- Regularization is essential to avoid overfitting, especially with complex models.
- Almost all type of regularization techniques showed similar improvement in model performance.

---

### Error Analysis and Model Refinement

An in-depth error analysis was conducted to diagnose model performance and identify areas for improvement. Based on the analysis, adjustments were made to further refine the models and improve accuracy.

**Key Takeaways:**
- Error analysis is vital for identifying and correcting model weaknesses.

---

### Finish: Enhancing Performance with Ensemble Methods

Finally, we applied ensemble methods like Bagging and Boosting to combine multiple models and enhance overall performance. Techniques such as Random Forest and Gradient Boosting resulted in little improvements further.

![Ensemble Methods Report](/projects/carPrice/Day6-ensembling-Report.jpg)

**Models Used:**
- Bagging with Ridge regression
- AdaBoost with Ridge regression

**Key Insights:**
- Ensemble methods leverage the strengths of different models, leading to better predictions.

---

### Conclusion

Our systematic approach—progressing from data exploration and preprocessing to feature engineering, regularization, error analysis, and ensembling—resulted in a robust predictive model for car price estimation.


---